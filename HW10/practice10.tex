\documentclass{article}

\usepackage{courier}
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{float}
\usepackage[toc,page]{appendix}
\usepackage{dcolumn}
\usepackage{pdflscape}
\usepackage{hyperref}
\usepackage{framed}
\usepackage[english]{babel}
\usepackage{soul}
\usepackage{caption}
\usepackage{amsfonts}
\captionsetup[figure]{labelformat=empty}%

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

\newcommand{\footlabel}[2]{%
    \addtocounter{footnote}{1}%
    \footnotetext[\thefootnote]{%
        \addtocounter{footnote}{-1}%
        \refstepcounter{footnote}\label{#1}%
        #2%
    }%
    $^{\ref{#1}}$%
}

\newcommand{\footref}[1]{%
    $^{\ref{#1}}$%
}

%\usepackage[colorlinks]{hyperref}
\hypersetup{linkcolor=DarkRed}
\hypersetup{urlcolor=DarkBlue}
\usepackage{cleveref}

\title{Data Mining\\Homework Assignment \#10} % Title

\author{Dmytro Fishman, Anna Leontjeva and Jaak Vilo} % Author name

\begin{document}

\maketitle % Insert the title, author and date
From the lecture we know that in order to classify two-dimensional points using a linear classifier following classification rule must be applied for each point ($x_1$, $x_2$):
$$
f(x) = sign (\textbf{w}\cdot\textbf{x} + b) = sign(w_1x_1 + w_2x_2 + b),
$$ 
where ($w_1$, $w_2$, b) are the parameters that learned from the data (it should remind you of ($ax + by + c$).
\section*{Task 1}
Consider the training set \{((0,0), 1), ((1,0),1), ((2,3),-1), ((3,2),-1)\}, consisting of two-dimensional points, each one having class label 1 or $-1$. Depict the points on a plane (you can use function plot.data() from the attached code file or pen and paper) and sketch the location of the maximal margin separating hyperplane. Small recap from the lecture: margin is the distance to closest point in each class. The points with the smallest margin are called support vectors. 
\begin{itemize}
\item Which points are the support vectors?
\item How many support vectors you would expect to have on average (e.g. for randomly generated data points)? Explain.
\end{itemize}
\section*{Task 2}
Consider now another training set \{((1,1), -1), ((1,3),-1), ((2,5),-1), ((4,2),-1), ((5,5),1), ((5,9),1), ((6,1),-1), ((7,5),1), ((9,1),1), ((11,3),1), ((11,7),1)\}. You happened to know that 5th, 7th and 9th points are support vectors. Plot these data. Find maximal margin separating hyperplane from these support vectors and calculate corresponding margin. 
Hint:
\begin{itemize}
\item you will probably need to find the equation of the line that goes through points that belong to the same class. 
\item resulting separating hyperplane should be parallel to this line, explain why.
\item it should be equally distanced from all support vectors.
\end{itemize}
\section*{Task 3}
On the lecture we discussed the perceptron algorithm, which is a linear classifier that for a given dataset $\{(\textbf{x}_i,y_i)\},\textbf{x} \in\mathbb{R}^{2}, y_i  \in \{-1, 1\}$ finds a separating hyperplane in the following way:
\begin{itemize}
\item Start with  $\textbf{w} = (0,0)$ (let us assume that b also equals to 0 for simplicity). 
\item Find an item  $\{(\textbf{x}_j,y_j)\}$ for which, $sign(\textbf{w}\cdot\textbf{x}_j + b)\neq y_j$ and update the parameter  $\textbf{w}$ in the following way: $$\textbf{w}~=~\textbf{w}~+~\mu\sum_{j}{\textbf{x}_j y_j},$$
where $\mu$ is a learning rate of our algorithm.
\item Repeat this procedure until there is no such item, return \textbf{w}.
\end{itemize}
First, try to understand the text above. One of the possible implementation of this algorithm is given to you in the code.r. Your task will be to correct one line of this code to make it work as expected (all the instances are classified correctly). Include resulting plot into the report. 
\begin{itemize}
\item How many steps did it take to converge to the final solution?
\item Try changing $\mu$. Does anything change besides the scale?
\end{itemize}
\section*{Task 4}
Sauron case study again. In the last (not-official) homework assignment (\url{http://fouryears.eu/2009/11/21/machine-learning-in-mordor/}) you had to analyze and predict which student Aghargh or Bughrorc has better chances of not jumping from the tower. In this exercise will try to simulate Sauron's case ourselves.
Let us supposed that the second student got really lucky and guessed a proper representation for the spells. That is, indeed each spell can be represented as a set of letters. When a spell is chosen from the Great Book at random, each letter will appear independently with probability 0.3:
\begin{lstlisting}
make_one_random_spell = function() {
rbinom(26,1,0.3)
}
\end{lstlisting}
Let us also suppose that $\textit{true}$ classification of the spells can be achieved using a linear classifier:
\begin{lstlisting}
true_spell_class = function(spell) {
    w = c(1,-2,3,-4,5,-6,7,-8,9,-10,11,-12,13,-14,15,
          -16,17,-18,19,-20,21,-22,23,-24,25,-26)
    
    sign(spell %*% w - 27.5)
}
\end{lstlisting}

Finally, let us assume that the second student got even more lucky because for training he attempted to fit exactly the $\textit{linear}$ model to the data, using the SVM algorithm. For simplicity let us suppose that instead of doing the irrelevant training/testing set split, the student just used all 20 instances to
train the model.
In this case, the situation that happened with Sauron and the students can be simulated as follows:
\begin{lstlisting}
    # Sauron generates a dataset:
    spells = make_n_random_spells(20)
    c = true_spell_class(spells)
    
    # First student finds the majority class
    majority_class = sign(sum(c) + 0.5)
    
    # Second student trains an SVM:
    svm_model = svm(spells, c, type='C', kernel='linear')

    # Sauron generates a new example:
    test_example = make_one_random_spell()
    
    # .. and tests each student's predictions:
    student1_correct = (majority_class == true_spell_class(test_example))
    student2_correct = (predict(svm_model, t(test_example)) == true_spell_class(test_example))
\end{lstlisting}
Simulate this situation 1000 times or more, and observe:
\begin{itemize}
\item How often the first student guesses correctly (i.e. what is his method's expected generalization error)?
\item How often would the second student guess correctly?
\item In those cases when the predictions differ, how often is the first student right?
\end{itemize}
\section*{Task 5}
Could you somehow improve the second student's performance by, say, doing training an a smarter way, tunning method parameters or changing the number of attributes used in training? What happens when the dataset becomes larger (i.e. increase  $\textit{n}$ to 30, 40, ... , 100, 200, ...)?

\section*{Task 6}
In this exercise we shall use the chord sequences of some popular songs by $\textit{The Beatles}$, $\textit{Paul McCartney}$, $\textit{Eric Clapton}$, $\textit{Red Hot Chili Peppers}$, $\textit{Metallica}$ and $\textit{Nirvana}$ from the website \url{http://www.e-chords.com}. Analyze this dataset using whatever machine learning or data mining methods you find applicable. As a minimum, you could check whether it is possible to discriminate the artists on the basis of the chord sequences. However, you need not limit yourself to classification only. Other options, such as clustering, automated chord sequence generation, frequent pattern detection, association rule mining or visualization might be rather exciting. Report your findings.
\end{document}
